{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49e6586",
   "metadata": {},
   "source": [
    "# Contextual Transformer Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41597d1",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "In this notebook, I fine-tune a pretrained Transformer model for stress detection from social media posts. The goal is to compare a contextual model against the interpretable feature-based baseline, using the same train/test split strategy and a robust evaluation setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7d26da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import kagglehub\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99a3cec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_id</th>\n",
       "      <th>sentence_range</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>confidence</th>\n",
       "      <th>social_timestamp</th>\n",
       "      <th>social_karma</th>\n",
       "      <th>syntax_ari</th>\n",
       "      <th>...</th>\n",
       "      <th>lex_dal_min_pleasantness</th>\n",
       "      <th>lex_dal_min_activation</th>\n",
       "      <th>lex_dal_min_imagery</th>\n",
       "      <th>lex_dal_avg_activation</th>\n",
       "      <th>lex_dal_avg_imagery</th>\n",
       "      <th>lex_dal_avg_pleasantness</th>\n",
       "      <th>social_upvote_ratio</th>\n",
       "      <th>social_num_comments</th>\n",
       "      <th>syntax_fk_grade</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>896</td>\n",
       "      <td>relationships</td>\n",
       "      <td>7nu7as</td>\n",
       "      <td>[50, 55]</td>\n",
       "      <td>Its like that, if you want or not.“ ME: I have...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1514980773</td>\n",
       "      <td>22</td>\n",
       "      <td>-1.238793</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.2000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.65864</td>\n",
       "      <td>1.32245</td>\n",
       "      <td>1.80264</td>\n",
       "      <td>0.63</td>\n",
       "      <td>62</td>\n",
       "      <td>-0.148707</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19059</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>680i6d</td>\n",
       "      <td>(5, 10)</td>\n",
       "      <td>I man the front desk and my title is HR Custom...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1493348050</td>\n",
       "      <td>5</td>\n",
       "      <td>7.684583</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4000</td>\n",
       "      <td>1.1250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.69133</td>\n",
       "      <td>1.69180</td>\n",
       "      <td>1.97249</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2</td>\n",
       "      <td>7.398222</td>\n",
       "      <td>-0.065909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7977</td>\n",
       "      <td>ptsd</td>\n",
       "      <td>8eeu1t</td>\n",
       "      <td>(5, 10)</td>\n",
       "      <td>We'd be saving so much money with this new hou...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1524516630</td>\n",
       "      <td>10</td>\n",
       "      <td>2.360408</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1429</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.70974</td>\n",
       "      <td>1.52985</td>\n",
       "      <td>1.86108</td>\n",
       "      <td>1.00</td>\n",
       "      <td>8</td>\n",
       "      <td>3.149288</td>\n",
       "      <td>-0.036818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1214</td>\n",
       "      <td>ptsd</td>\n",
       "      <td>8d28vu</td>\n",
       "      <td>[2, 7]</td>\n",
       "      <td>My ex used to shoot back with \"Do you want me ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1524018289</td>\n",
       "      <td>5</td>\n",
       "      <td>5.997000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.3000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.72615</td>\n",
       "      <td>1.52000</td>\n",
       "      <td>1.84909</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7</td>\n",
       "      <td>6.606000</td>\n",
       "      <td>-0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1965</td>\n",
       "      <td>relationships</td>\n",
       "      <td>7r1e85</td>\n",
       "      <td>[23, 28]</td>\n",
       "      <td>I haven’t said anything to him yet because I’m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1516200171</td>\n",
       "      <td>138</td>\n",
       "      <td>4.649418</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1250</td>\n",
       "      <td>1.1429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.75642</td>\n",
       "      <td>1.43582</td>\n",
       "      <td>1.91725</td>\n",
       "      <td>0.84</td>\n",
       "      <td>70</td>\n",
       "      <td>4.801869</td>\n",
       "      <td>0.141667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id      subreddit post_id sentence_range  \\\n",
       "0    896  relationships  7nu7as       [50, 55]   \n",
       "1  19059        anxiety  680i6d        (5, 10)   \n",
       "2   7977           ptsd  8eeu1t        (5, 10)   \n",
       "3   1214           ptsd  8d28vu         [2, 7]   \n",
       "4   1965  relationships  7r1e85       [23, 28]   \n",
       "\n",
       "                                                text  label  confidence  \\\n",
       "0  Its like that, if you want or not.“ ME: I have...      0         0.8   \n",
       "1  I man the front desk and my title is HR Custom...      0         1.0   \n",
       "2  We'd be saving so much money with this new hou...      1         1.0   \n",
       "3  My ex used to shoot back with \"Do you want me ...      1         0.5   \n",
       "4  I haven’t said anything to him yet because I’m...      0         0.8   \n",
       "\n",
       "   social_timestamp  social_karma  syntax_ari  ...  lex_dal_min_pleasantness  \\\n",
       "0        1514980773            22   -1.238793  ...                    1.0000   \n",
       "1        1493348050             5    7.684583  ...                    1.4000   \n",
       "2        1524516630            10    2.360408  ...                    1.1429   \n",
       "3        1524018289             5    5.997000  ...                    1.0000   \n",
       "4        1516200171           138    4.649418  ...                    1.1250   \n",
       "\n",
       "   lex_dal_min_activation  lex_dal_min_imagery  lex_dal_avg_activation  \\\n",
       "0                  1.2000                  1.0                 1.65864   \n",
       "1                  1.1250                  1.0                 1.69133   \n",
       "2                  1.0000                  1.0                 1.70974   \n",
       "3                  1.3000                  1.0                 1.72615   \n",
       "4                  1.1429                  1.0                 1.75642   \n",
       "\n",
       "   lex_dal_avg_imagery  lex_dal_avg_pleasantness  social_upvote_ratio  \\\n",
       "0              1.32245                   1.80264                 0.63   \n",
       "1              1.69180                   1.97249                 1.00   \n",
       "2              1.52985                   1.86108                 1.00   \n",
       "3              1.52000                   1.84909                 1.00   \n",
       "4              1.43582                   1.91725                 0.84   \n",
       "\n",
       "   social_num_comments  syntax_fk_grade  sentiment  \n",
       "0                   62        -0.148707   0.000000  \n",
       "1                    2         7.398222  -0.065909  \n",
       "2                    8         3.149288  -0.036818  \n",
       "3                    7         6.606000  -0.066667  \n",
       "4                   70         4.801869   0.141667  \n",
       "\n",
       "[5 rows x 116 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = kagglehub.dataset_download(\"monishakant/dataset-for-stress-analysis-in-social-media\")\n",
    "csv_path = os.path.join(path, \"dreaddit_StressAnalysis - Sheet1.csv\")\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf2c4215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 143)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = raw text, y = labels\n",
    "train_idx_path = \"../data/train_idx.npy\"\n",
    "test_idx_path = \"../data/test_idx.npy\"\n",
    "\n",
    "if os.path.exists(train_idx_path) and os.path.exists(test_idx_path):\n",
    "    train_idx = np.load(train_idx_path)\n",
    "    test_idx = np.load(test_idx_path)\n",
    "else:\n",
    "    # Fallback: deterministic split if index files are missing\n",
    "    all_idx = df.index.to_numpy()\n",
    "    labels = df[\"label\"].astype(int)\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        all_idx,\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=labels,\n",
    "    )\n",
    "    np.save(train_idx_path, train_idx)\n",
    "    np.save(test_idx_path, test_idx)\n",
    "\n",
    "X_train = df.loc[train_idx, \"text\"].astype(str).tolist()\n",
    "X_test  = df.loc[test_idx, \"text\"].astype(str).tolist()\n",
    "\n",
    "y_train = df.loc[train_idx, \"label\"].astype(int).tolist()\n",
    "y_test  = df.loc[test_idx, \"label\"].astype(int).tolist()\n",
    "\n",
    "len(X_train), len(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a359ba08",
   "metadata": {},
   "source": [
    "## Model Choice:\n",
    "\n",
    "I start with a compact pretrained model (e.g., DistilBERT) to keep training efficient while still benefiting from contextual representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30b57d9",
   "metadata": {},
   "source": [
    "## Tokenization + Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f267d393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497f73251db84995a898cfa94aa60ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947c9b434b7a4b83957d0e65f3fe591b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install dependencies once from terminal:\n",
    "# pip install -r requirements.txt\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_ds = Dataset.from_dict({\"text\": X_train, \"label\": y_train})\n",
    "test_ds  = Dataset.from_dict({\"text\": X_test,  \"label\": y_test})\n",
    "\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "train_tok = train_ds.map(tokenize, batched=True)\n",
    "test_tok  = test_ds.map(tokenize, batched=True)\n",
    "\n",
    "train_tok = train_tok.remove_columns([\"text\"])\n",
    "test_tok  = test_tok.remove_columns([\"text\"])\n",
    "\n",
    "train_tok.set_format(\"torch\")\n",
    "test_tok.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ff9635",
   "metadata": {},
   "source": [
    "## Fine-Tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8124a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8fb4f1f2d74127aaf4862ced12d0a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification LOAD REPORT from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "classifier.bias         | MISSING    | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "classifier.weight       | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n",
      "/Users/jacopoamoretti/Desktop/-Universita Bologna-/2025_2026/AI in Industry/Progetto/dreaddit-stress-nlp/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 01:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.610272</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.600215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.614582</td>\n",
       "      <td>0.521046</td>\n",
       "      <td>0.734266</td>\n",
       "      <td>0.733209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.614582</td>\n",
       "      <td>0.512206</td>\n",
       "      <td>0.741259</td>\n",
       "      <td>0.740446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67672b5f1fc47b0be7eb2972c3cc2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacopoamoretti/Desktop/-Universita Bologna-/2025_2026/AI in Industry/Progetto/dreaddit-stress-nlp/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a7ce0be910428cbe3566d0b77d54e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacopoamoretti/Desktop/-Universita Bologna-/2025_2026/AI in Industry/Progetto/dreaddit-stress-nlp/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91bb079e260044da8c7ba654a3baf864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['distilbert.embeddings.LayerNorm.beta', 'distilbert.embeddings.LayerNorm.gamma'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=96, training_loss=0.5310603181521097, metrics={'train_runtime': 110.5014, 'train_samples_per_second': 13.574, 'train_steps_per_second': 0.869, 'total_flos': 49675274496000.0, 'train_loss': 0.5310603181521097, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1_macro\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"results/transformer_distilbert\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    seed=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=test_tok,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
